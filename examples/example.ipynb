{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7518c5c9-79ab-4daa-a7af-a5c699821c3a",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Nuggetizer: A lightweight nugget-based evaluation framework for pyterrier-rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c04970-bc5a-4e55-a669-fa19abcd1574",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Introduction\n",
    "- Objective: Demonstrate how to use NuggetizerRAG, a personal library for nugget-based evaluation in Retrieval-Augmented Generation (RAG).\n",
    "- Context: Inspired by the AutoNuggetizer framework from the TREC 2024 RAG Track.\n",
    "- Use case: Provide interpretable and automatic evaluation metrics for open-domain QA and generation tasks using PyTerrier pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5defa-adae-403b-a890-751d489416ab",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Motivation and Background\n",
    "- The Problem: Traditional RAG evaluations rely on lexical overlap or ROUGE scores, which miss semantic correctness.\n",
    "- The Solution: Nugget evaluation, originally proposed in TREC QA 2003, revived by AutoNuggetizer, uses semantically atomic facts (â€œnuggetsâ€) to evaluate answers.\n",
    "- Inspiration: This library reimplements a simplified, local version of AutoNuggetizer with modular hooks into PyTerrier and HuggingFace models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421df4f6-9dda-45b7-9de6-464cd3313fca",
   "metadata": {},
   "source": [
    "## âš™ï¸ Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c2daee-3b1c-4025-a1a4-bd18eca89b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/MattiWe/ir_datasets.git@add-msmarco-v2.1-trec-rag\n",
      "  Cloning https://github.com/MattiWe/ir_datasets.git (to revision add-msmarco-v2.1-trec-rag) to /tmp/pip-req-build-d6j4viou\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/MattiWe/ir_datasets.git /tmp/pip-req-build-d6j4viou\n",
      "  Running command git checkout -b add-msmarco-v2.1-trec-rag --track origin/add-msmarco-v2.1-trec-rag\n",
      "  Switched to a new branch 'add-msmarco-v2.1-trec-rag'\n",
      "  Branch 'add-msmarco-v2.1-trec-rag' set up to track remote branch 'add-msmarco-v2.1-trec-rag' from 'origin'.\n",
      "  Resolved https://github.com/MattiWe/ir_datasets.git to commit bd018b783e3d25942b69290f7be19eeb929022c2\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (4.13.4)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (2.6.0)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (5.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.22.0 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.38.0 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (4.67.1)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (4.4.4)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (0.2.5)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (0.1.9)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (3.3.0)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (0.2.3)\n",
      "Requirement already satisfied: pyarrow>=16.1.0 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from ir_datasets==0.5.10) (20.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir_datasets==0.5.10) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir_datasets==0.5.10) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from requests>=2.22.0->ir_datasets==0.5.10) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from requests>=2.22.0->ir_datasets==0.5.10) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from requests>=2.22.0->ir_datasets==0.5.10) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from requests>=2.22.0->ir_datasets==0.5.10) (2025.4.26)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /mnt/primary/conda-envs/nuggetizer/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir_datasets==0.5.10) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/MattiWe/ir_datasets.git@add-msmarco-v2.1-trec-rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcda75f3-f54f-46c1-8635-6888b41002fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q python-terrier pyterrier_t5 pyterrier_pisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe1c6a1-8a49-436f-80b6-39b1634c9ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/terrierteam/pyterrier_rag.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7275f3bc-b007-4ae6-a04b-a906c83be541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/mnt/primary/launcher-cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --no-deps ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe713d5c-c5f6-4f84-8429-740f2cab3948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "from pyterrier_rag.backend import Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11648e98-66ab-487f-998e-fab8d232c8ba",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c43bcb-63d3-40af-98d5-0918f370ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load('msmarco-segment-v2.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94859fe6-63f4-495a-b51c-18058d795fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_dataset = pt.get_dataset(\"irds:msmarco-segment-v2.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938bc72d-8861-4f58-a2b6-6b0a4f2c4cc4",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da2c9fa9-963a-4370-9c9f-1c805bed29e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_pisa import PisaIndex\n",
    "from pyterrier_t5 import MonoT5ReRanker\n",
    "\n",
    "def rename_segment(run):\n",
    "    run = run.rename(columns={\"segment\": \"text\"})\n",
    "    return run\n",
    "rename_pipe = pt.apply.generic(rename_segment)\n",
    "\n",
    "index = PisaIndex('/mnt/indices/msmarco-segment-v2.1.pisa/')\n",
    "bm25_ret = index.bm25() >> pt.text.get_text(pt_dataset, \"segment\") >> rename_pipe\n",
    "monoT5 = MonoT5ReRanker(batch_size=64, verbose=False)\n",
    "monoT5_ret = bm25_ret % 10 >> monoT5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf39de-7eb2-4c4d-95d7-567d2223aaa9",
   "metadata": {},
   "source": [
    "# Building a baseline retrieval run to generate baseline nuggets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f7a963-1f70-4b5f-85c3-820f8089e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../diversification/diversy-rag/datasets/TREC-RAGgy/raggy-dev.tsv\", sep=\"\\t\", dtype=str)\n",
    "topics_df  = df[[\"qid\", \"query\"]]\n",
    "answers_df = df[[\"qid\", \"query\", \"gold_answer\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1526ac-11ef-4753-9a14-13e00e4e1e7d",
   "metadata": {},
   "source": [
    "baseline = (monoT5_ret)(topics_df.head(10))\n",
    "baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b0d35-68e3-4918-97a9-440209c0d7b3",
   "metadata": {},
   "source": [
    "# Nuggetizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d428aa08-c6ce-4f9f-962d-e69791e08f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:05<00:00,  1.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from pyterrier_rag.backend import HuggingFaceBackend\n",
    "\n",
    "backend =  HuggingFaceBackend(\"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\",\n",
    "                                          max_new_tokens=2048,\n",
    "                                          model_args={\n",
    "                                              \"device_map\": \"cuda\"\n",
    "                                          }\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0516ff-760b-46d0-b1c7-ccc3e3fd9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastchat.conversation import register_conv_template, get_conv_template, Conversation, SeparatorStyle\n",
    "\n",
    "register_conv_template(\n",
    "    Conversation(\n",
    "        name=\"meta-llama-3.1-sp\",\n",
    "        system_message=\"\",\n",
    "        roles=(\"user\", \"assistant\"),\n",
    "        sep_style=SeparatorStyle.ADD_COLON_SINGLE,\n",
    "        sep=\"\\n\",\n",
    "        messages=[],\n",
    "    )\n",
    ")\n",
    "\n",
    "conv_template = get_conv_template(\"meta-llama-3.1-sp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15180598-f175-4882-8c1b-44232c8a76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_csv(path, content):\n",
    "    content.to_csv(path, index=False)\n",
    "\n",
    "def load_csv(path):\n",
    "    try:\n",
    "        content = pd.read_csv(path)\n",
    "        return content\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec0c3eb-3896-4f50-adc8-eaef610d51af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pt.apply.by_query():   0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:12 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?window/s]\u001b[A/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " ['You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 2 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: are landlords liable if someone breaks in a hurts tenant\\nNugget List: [\\'Landlord not liable for conditions arising after tenant takes possession\\', \"Landlord can be held liable for tenant\\'s behavior if aware and does nothing\"]\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n",
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.18s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " ['You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: are landlords liable if someone breaks in a hurts tenant\\nNugget List: [\\'Landlord liable for injuries on rental property\\', \\'Tenant must prove landlord negligence\\', \\'Landlord must maintain common areas\\', \\'Tenant can sue for medical bills and lost earnings\\', \"Landlord liable for injuries caused by tenant\\'s dog\", \\'Landlord must know dog is dangerous\\', \\'Landlord must have power to remove dog\\', \\'Landlord liable for injuries off rental property\\', \\'Landlord must maintain property in safe condition\\', \\'Landlord liable for faulty wiring and toxic mold\\']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.02s/window]\u001b[A\n",
      "pt.apply.by_query():   6%|â–Œ         | 13/225 [00:10<02:43,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:21 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?window/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 1 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: average age of men at marriage\\nNugget List: ['average age of men at marriage has increased in the last 60 years']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.57s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: average age of men at marriage\\nNugget List: ['average age of men at marriage in Western Europe is around 30 years', 'average age of men at marriage has increased over the past two decades', 'average age of men at marriage has increased by two years since 1980', 'average age of men at marriage is higher in countries with higher social status', 'average age of men at marriage is higher in countries with better education system', 'average age of men at marriage is higher in countries with better employment', 'average age of men at marriage is higher in wealthier countries', 'average age of men at marriage is around 28-30 years old', 'average age of men at marriage is around 25-35 years old', 'ideal age for men to get married is 27 years']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:08<00:04,  4.52s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: average age of men at marriage\\nNugget List: ['average age of men at marriage is 26.8 years', 'average age of men at marriage in US is 26.8 years', 'average age of men at marriage in UK is 30.8 years', 'average age of men at marriage in Alabama is 25.5 years', 'average age of men at marriage in District of Columbia is 30 years', 'average age of men at marriage in Moldova is 26 years', 'average age of men at marriage in Mexico is 23.3 years', 'average age of men at marriage in Europe varies by country', 'average age of men at marriage in Northern Europe is around 30 years', 'average age of men at marriage in Southern Europe is around 30 years']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:14<00:00,  4.87s/window]\u001b[A\n",
      "pt.apply.by_query():  15%|â–ˆâ–Œ        | 34/225 [00:24<02:17,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:30 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?window/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: crest syndrome esophageal dysfunction\\nNugget List: ['CENP-C', 'Connective tissue disease', 'Scleroderma variant', 'Clinical signs', 'Laboratory test', 'Diagnostic criteria', 'Prognosis', 'Treatment options', 'Complications', 'Pulmonary function testing']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:06<00:12,  6.31s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: crest syndrome esophageal dysfunction\\nNugget List: ['Antinuclear antibodies', 'Centromere antibodies', 'Esophageal hypomotility', 'Reflux esophagitis', 'Nonpitting digital edema', 'Pulmonary hypertension', 'Biliary cirrhosis', 'HLA-DR1', 'CENP-A', 'CENP-B']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.53s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " ['You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: crest syndrome esophageal dysfunction\\nNugget List: [\\'CREST syndrome\\', \\'Calcinosis cutis\\', \"Raynaud\\'s phenomenon\", \\'Esophageal dysfunction\\', \\'Sclerodactyly\\', \\'Telangiectasia\\', \\'Systemic sclerosis\\', \\'Limited scleroderma\\', \\'Diffuse scleroderma\\', \\'Autoimmune disease\\']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  6.51s/window]\u001b[A\n",
      "pt.apply.by_query():  28%|â–ˆâ–ˆâ–Š       | 64/225 [00:44<01:49,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:21 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?window/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 1 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: does light intensity or concentration of carbon dioxide have a higher rate of photosynthesis\\nNugget List: ['Photosynthesis rate levels off at high light intensities and carbon dioxide concentrations']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:01<00:03,  1.63s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: does light intensity or concentration of carbon dioxide have a higher rate of photosynthesis\\nNugget List: ['Chlorophyll concentration affects photosynthesis rate', 'Carbon dioxide concentration limits photosynthesis rate at high light intensities', 'Carbon dioxide concentration of 0.03 to 0.04 percent is sufficient for photosynthesis', 'Carbon dioxide concentration above 0.5 percent is injurious to plants', 'Light intensity is limiting factor at low light intensities', 'Carbon dioxide concentration is limiting factor at high light intensities', 'Temperature is limiting factor at low temperatures', 'Carbon dioxide concentration affects photosynthesis rate at low concentrations', 'Carbon dioxide concentration does not affect photosynthesis rate at high concentrations', 'Light intensity and carbon dioxide concentration have an optimum level']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:08<00:04,  4.86s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: does light intensity or concentration of carbon dioxide have a higher rate of photosynthesis\\nNugget List: ['Light intensity affects photosynthesis rate', 'Carbon dioxide concentration affects photosynthesis rate', 'Temperature affects photosynthesis rate', 'Higher light intensity increases photosynthesis rate', 'Higher carbon dioxide concentration increases photosynthesis rate', 'Higher temperature increases photosynthesis rate', 'Optimum temperature range is 25 to 35 o C', 'Water availability affects photosynthesis rate', 'Carbon dioxide is essential for photosynthesis', 'Light wavelength affects photosynthesis rate']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.29s/window]\u001b[A\n",
      "pt.apply.by_query():  38%|â–ˆâ–ˆâ–ˆâ–Š      | 85/225 [01:00<01:39,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:30 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?window/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " ['You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how does my baby get submitted for medicaid after birth\\nNugget List: [\\'full Medicaid coverage\\', \\'Medicaid managed care plan\\', \"baby\\'s Medicaid number\", \"baby\\'s Medicaid ID number\", \"baby\\'s Medicaid ID card\", \"baby of mother\\'s name\", \\'card control number\\', \\'Medical Assistance Referral Form\\', \\'Unborn Activation Form\\', \\'proof of eligibility\\']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:07<00:14,  7.31s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " ['You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how does my baby get submitted for medicaid after birth\\nNugget List: [\"baby\\'s plan\", \\'MMA plan enrollment\\', \\'Medicaid services\\', \\'fee for service\\', \\'Medicaid fiscal agent\\', \"baby\\'s eligibility\", \\'MMA plan\\', \"mother\\'s eligibility category\", \\'MU\\', \\'FP\\']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:14<00:07,  7.04s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " ['You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how does my baby get submitted for medicaid after birth\\nNugget List: [\"baby\\'s Medicaid eligibility\", \"mother\\'s Medicaid eligibility\", \\'Medicaid managed care plan\\', \\'newborn activation request\\', \\'Florida Medicaid Secure Web Portal\\', \\'Florida Health Plan Portal\\', \\'Medicaid fiscal agent\\', \"baby\\'s name, gender, and birth date\", \\'new Medicaid gold card\\', \"mother\\'s plan\"]\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:21<00:00,  7.06s/window]\u001b[A\n",
      "pt.apply.by_query():  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 115/225 [01:21<01:17,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:20 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?window/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how is the oil pollution act of 1990 effect oil companies\\nNugget List: ['Enhanced federal response capability', 'Increased potential liabilities', 'Limited liability for companies to $75 million', 'Prevented oil spills from vessels and facilities', 'Assigned liability for cleanup and damage costs', 'Defined responsible parties and financial liability', 'Implemented processes for measuring damages', 'Specified damages for which violators are liable', 'Established fund for damages, cleanup, and removal costs', 'Resulted in changes to oil production, transportation, and distribution industries']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.89s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how is the oil pollution act of 1990 effect oil companies\\nNugget List: ['Oil Pollution Act of 1990', 'Passed by 101st US Congress', 'Signed by President George H.W. Bush', 'Effective August 18, 1990', 'Amended Clean Water Act', 'Increased penalties for oil spills', 'Required double hulls for oil tankers', 'Established Oil Spill Liability Trust Fund', 'Provided financial responsibility requirements', 'Mandated contingency planning']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:15<00:00,  7.74s/window]\u001b[A\n",
      "pt.apply.by_query():  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 135/225 [01:36<01:05,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:24 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?window/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 4 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how long does it take for a sprained wrist to heal\\nNugget List: ['sprained pinky healing time 2-3 weeks', 'AC sprain healing time 4-6 weeks', 'sprained rib ligament healing time 3-6 weeks', 'sprained foot healing time varies']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:03<00:06,  3.34s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how long does it take for a sprained wrist to heal\\nNugget List: ['mild sprain heals 1 month over 50', 'sprained wrist ligament healing time 2-10 weeks', 'sprained wrist healing time 2-3 weeks to be normal', 'sprained wrist healing time 5-6 days with rest', 'sprained hand healing time 7-10 days', 'sprained foot healing time 1 week', 'sprained arm healing time 2-3 weeks', 'sprained knee healing time few weeks to months', 'sprained ankle healing time 1 week', 'sprained toe healing time couple weeks']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:10<00:05,  5.67s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how long does it take for a sprained wrist to heal\\nNugget List: ['wrist sprain healing time 2-10 weeks', 'mild wrist sprain heals 2-3 days', 'moderate wrist sprain heals 1-2 weeks', 'severe wrist sprain heals several weeks to months', 'rest ice compression helps wrist sprain healing', 'wrist sprain occurs due to ligament injury', 'wrist sprain common due to falling', 'sprained wrist symptoms swelling stiffness', 'wrist sprain healing time varies with age', 'mild sprain heals 10 days under 25']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.90s/window]\u001b[A\n",
      "pt.apply.by_query():  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 159/225 [01:54<00:48,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:18 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?window/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 8 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how many years in jail for money laundering\\nNugget List: ['Money laundering is a felony offense', 'Money laundering is a white-collar crime', 'Money laundering involves deceit and financial gain', 'First-degree money laundering is a class 2 felony', 'Second-degree money laundering is a class 3 felony', 'Money laundering penalties depend on prior convictions', 'Money laundering can be charged as a misdemeanor or felony in California', 'RICO charges may be added in serious cases']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.03s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how many years in jail for money laundering\\nNugget List: ['Money laundering jail time depends on amount laundered', 'Texas penal code requires 180 days to 2 years in jail for $1,500 to $20,000', '2 to 10 years in prison for $20,000 to $100,000', '2 to 20 years in prison for $100,000 to $200,000', '5 to 99 years in prison for over $100,000', 'California misdemeanor money laundering carries up to 1 year in jail', 'California felony money laundering carries 16 months to 4 years in jail', 'Federal money laundering carries up to 20 years in prison', 'Fines for money laundering vary by state and federal cases', 'Repeat offenders can face up to 35 years in jail']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:13<00:00,  6.96s/window]\u001b[A\n",
      "pt.apply.by_query():  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 177/225 [02:08<00:35,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:26 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?window/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 6 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how to help a jammed finger\\nNugget List: ['Ibuprofen reduces swelling', 'Follow dosage recommendations', 'Collateral ligaments support joint', 'Jammed finger not serious', 'At-home treatments help', 'Medical treatments help']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:04<00:09,  4.84s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " ['You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how to help a jammed finger\\nNugget List: [\\'Rest is key\\', \\'Ice for 20 minutes\\', \\'Repeat as needed\\', \\'Avoid slamming doors\\', \\'Finger protection strips help\\', \\'Door guards help\\', \"Check children\\'s hands\", \\'Teach children door safety\\', \\'Arthritis symptoms similar\\', \\'No pulling a jammed finger\\']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:11<00:06,  6.11s/window]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: how to help a jammed finger\\nNugget List: ['Jammed finger causes pain', 'Swelling and stiffness', 'Difficulty moving finger', 'Ice pack helps', 'Epsom salt helps', 'Aloe Vera Gel helps', 'Apple Cider Vinegar helps', 'Turmeric helps', 'Tape injured finger', 'Immobilize finger']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  6.34s/window]\u001b[A\n",
      "pt.apply.by_query(): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225/225 [02:27<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN:23 - 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?window/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 3 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: information about who howard gardner and what does he do\\nNugget List: ['He has been the co-director of The Good Project since 1995', 'Gardner retired from teaching in 2019', 'He published his intellectual memoir A Synthesizing Mind in 2020']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:02<00:05,  2.91s/window]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " ['You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: information about who howard gardner and what does he do\\nNugget List: [\"Gardner\\'s theory of multiple intelligences includes eight different types of intelligences\", \\'Linguistic-Verbal intelligence\\', \\'Logical-Mathematical intelligence\\', \\'Visual-Spatial intelligence\\', \\'Bodily-Kinesthetic intelligence\\', \\'Musical-Rhythmic intelligence\\', \\'Interpersonal intelligence\\', \\'Intrapersonal intelligence\\', \\'Naturalistic intelligence\\', \\'Gardner has been a member of organizations such as the American Academy of Arts and Sciences\\']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:09<00:05,  5.26s/window]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " [\"You are NuggetizeScoreLLM, an intelligent assistant that can label a list of atomic nuggets based on their importance for a given search query.\\nuser: Based on the query, label each of the 10 nuggets either a vital or okay based on the following criteria. Vital nuggets represent concepts that must be present in a â€œgoodâ€ answer; on the other hand, okay nuggets contribute worthwhile information about the target but are not essential. Return the list of labels in a Pythonic list format (type: List[str]). The list should be in the same order as the input nuggets. Make sure to provide a label for each nugget.\\n\\nSearch Query: information about who howard gardner and what does he do\\nNugget List: ['Howard Gardner is an American psychologist', 'He specializes in cognitive and developmental psychology', 'He is best known for his theory of multiple intelligences', 'Gardner believes that the way people usually think about intelligence is too narrow', 'He was born on July 11, 1943 in Scranton, Pennsylvania', 'His parents were Ralph and Hilde Gardner', 'They were German-Jewish immigrants who fled from Nazi persecution in Germany', 'Gardner is the John H. and Elisabeth A. Hobbs Research Professor of Cognition and Education at Harvard University', 'He is the senior director of Harvard Project Zero', 'He has written hundreds of research articles and thirty books']\\n\\nOnly return the list of labels (List[str]). Do not explain.\\nLabels:\\n\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.92s/window]\n"
     ]
    }
   ],
   "source": [
    "from open_nuggetizer.nuggetizer import Nuggetizer\n",
    "\n",
    "nuggetizer = Nuggetizer(\n",
    "    backend=backend, \n",
    "    conversation_template=conv_template,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "nuggets = load_csv(\"nuggets.csv\")\n",
    "if nuggets is None:\n",
    "    nuggets = nuggetizer.create(baseline)\n",
    "    save_csv(\"nuggets.csv\", nuggets)\n",
    "\n",
    "scored_nuggets = load_csv(\"scored_nuggets\")\n",
    "if scored_nuggets is None:\n",
    "    scored_nuggets = nuggetizer.score(nuggets)\n",
    "    save_csv(\"scored_nuggets.csv\", scored_nuggets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221e6935-f55d-4b68-a9cf-e517e9f20727",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8df853-4120-442c-ad08-6ccd743bbc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>qid</th>\n",
       "      <th>query_0</th>\n",
       "      <th>qanswer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A chat between a curious human and an artifici...</td>\n",
       "      <td>23287</td>\n",
       "      <td>are landlords liable if someone breaks in a hu...</td>\n",
       "      <td>Landlords are generally not liable for injuri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt    qid  \\\n",
       "0  A chat between a curious human and an artifici...  23287   \n",
       "\n",
       "                                             query_0  \\\n",
       "0  are landlords liable if someone breaks in a hu...   \n",
       "\n",
       "                                             qanswer  \n",
       "0   Landlords are generally not liable for injuri...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier_rag.prompt import Concatenator\n",
    "from pyterrier_rag.readers import Reader\n",
    "\n",
    "reader = Reader(backend)\n",
    "rag_pipeline = monoT5_ret % 3 >> Concatenator() >> reader\n",
    "\n",
    "(monoT5_ret % 3 >> Concatenator() >> reader)(topics_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea85e93-6c63-4105-9f48-58be6b4d8acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'qid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21963/1208959763.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyterrier_rag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m results = pt.Experiment(\n\u001b[0m\u001b[1;32m      4\u001b[0m     [\n\u001b[1;32m      5\u001b[0m         \u001b[0mrag_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     ],\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/pyterrier/pipelines.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(retr_systems, topics, qrels, eval_metrics, names, perquery, dataframe, batch_size, filter_by_qrels, filter_by_topics, baseline, test, correction, correction_alpha, highlight, round, verbose, save_dir, save_mode, save_format, precompute_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unrecognised save_mode %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0msave_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s.%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_ext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             time, evalMeasuresDict = _run_and_evaluate(\n\u001b[0m\u001b[1;32m    653\u001b[0m                 \u001b[0msystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m                 \u001b[0mperquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mperquery\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/pyterrier/pipelines.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(system, topics, qrels, metrics, pbar, save_mode, save_file, save_format, perquery, batch_size, backfill_qids)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d topics, but no results received from %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         evalMeasuresDict = _ir_measures_to_dict(\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0mir_measures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_irmeasures_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mrev_mapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/pyterrier/pipelines.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(seq, measures, rev_mapping, num_q, perquery, backfill_qids)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrtr_perquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mbackfill_qids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"backfill_qids only supported when perquery=True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mmetric_agg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mrev_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mmeasure_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrev_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mmetric_agg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeasure_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/ir_measures/providers/base.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, run)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \"\"\"\n\u001b[1;32m     20\u001b[0m         \u001b[0mYields\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtopic\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[1;32m     22\u001b[0m         \u001b[0mexpected_measure_qids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqrel_qids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mexpected_measure_qids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_measure_qids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/ir_measures/providers/runtime_provider.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, run)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0msort_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0msort_orders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_orders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmeasure\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/open_nuggetizer/measure/_ir_measures.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, qrels, run)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mruntime_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0massignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nuggetizer_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNuggetEvalProvider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqrels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0massignments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRAGRunConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict_of_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_calc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/open_nuggetizer/nuggetizer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, run, qrels)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnugget\u001b[0m \u001b[0massignments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \"\"\"\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqrels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"qid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"qid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10829\u001b[0m     ) -> DataFrame:\n\u001b[1;32m  10830\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10832\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/nuggetizer/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'qid'"
     ]
    }
   ],
   "source": [
    "import pyterrier_rag.measures\n",
    "\n",
    "results = pt.Experiment(\n",
    "    [\n",
    "        rag_pipeline\n",
    "    ],\n",
    "    topics_df.head(2), \n",
    "    answers_df,\n",
    "    [pyterrier_rag.measures.F1, nuggetizer.VitalScore()],\n",
    "    #batch_size=25,\n",
    "    names=['baseline retriever'],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nuggetizer]",
   "language": "python",
   "name": "conda-env-nuggetizer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
